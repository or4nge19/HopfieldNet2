\chapter{NN}

\begin{definition}\label{Neural Network}
\lean{NeuralNetwork}
An (artificial) neural network is a directed graph \( G = (U, C) \), where neurons \( u \in U \) 
are connected by directed edges \( c \in C \) (connections). 
The neuron set is partitioned as \( U = U_{\mathrm{in}} \cup U_{\mathrm{out}} \cup U_{\mathrm{hidden}} \), 
with \( U_{\mathrm{in}}, U_{\mathrm{out}} \neq \emptyset \) and \( U_{\mathrm{hidden}} \cap (U_{\mathrm{in}} 
\cup U_{\mathrm{out}}) = \emptyset \). Each connection \( (v, u) \in C \) has a weight \( w_{uv} \), and each neuron \( u \) 
has real-valued quantities: network input \( \mathrm{net}_u \), activation \( \mathrm{act}_u \), and output \( \mathrm{out}_u \).
Input neurons \( u \in U_{\mathrm{in}} \) also have a fourth quantity, the external input \( \mathrm{ext}_u \). 
The predecessors and successors of a vertex \( u \) in a directed graph \( G = (U, C) \) are defined as
$\mathrm{pred}(u) = \{ v \in V \mid (v, u) \in C \}$ and 
$\mathrm{succ}(u) = \{ v \in V \mid (u, v) \in C \}$ respectively. Each neuron \( u \) 
is associated with the following functions:  

$$f_{\mathrm{net}}^{(u)} : \mathbb{R}^{2|\mathrm{pred}(u)|+ \kappa_1 (u)} \to \mathbb{R}, \quad
 f_{\mathrm{act}}^{(u)} : \mathbb{R}^{1+\kappa_2 (u)} \to \mathbb{R}, \quad f_{\mathrm{out}}^{(u)} : \mathbb{R} \to \mathbb{R}. $$
 These functions compute \( \mathrm{net}_u \), \( \mathrm{act}_u \), and \( \mathrm{out}_u \), 
 where \( \kappa_1(u) \) and \( \kappa_2(u) \) count the number of parameters of those functions, 
 which can depend on the neurons. Specifically, the new activation $\mathrm{act}_u'$ of a neuron $u$ is computed as follows:
\begin{equation*}
\mathrm{act}_u'=  
f_{\mathrm{act}}^{(u)} \big(f_{\mathrm{net}}^{(u)} \big(
w_{uv_1}, \ldots, w_{uv_{\mathrm{pred}(u)}}, f_{\mathrm{out}}^{(v_1)}(\mathrm{act}_{v_1}),\ldots,
f_{\mathrm{out}}^{(v_{\mathrm{pred}(u)})}(\mathrm{act}_{v_{\mathrm{pred}(u)}}),
\boldsymbol{\sigma}^{(u)}\big), \boldsymbol{\theta}^{(u)}\big)
\end{equation*}
where $\boldsymbol{\sigma}^{(u)} = (\sigma_1^{(u)} , \ldots , 
\sigma_{\kappa_1(u)}^{(u)} )$ and $\boldsymbol{\theta} = (\theta_1^{(u)} , \ldots ,
 \theta_{\kappa_2(u)}^{(u)} )$ are the input parameter vectors.
\leanok
\end{definition}

\begin{definition}\label{Hopfield Network}
\lean{HopfieldNetwork}
\uses{NeuralNetwork}
\leanok
A Hopfield network is a neural network with graph $G = (U,C)$ as described in the previous section, 
that satisfies the following conditions:
\( U_{\text{hidden}} = \emptyset \), and \( U_{\text{in}} = U_{\text{out}} = U \),
 \( C = U \times U - \{(u, u) \mid u \in U \} \), i.e., no self-connections. 
The connection weights are symmetric, i.e., for all \( u, v \in U \), we have \( w_{uv} = w_{vu} \) when \( u \neq v \). 
The activation of each neuron is either \( 1 \) or \( -1 \) depending on the input. There are no loops, meaning neurons don’t 
receive their own output as input. Instead, each neuron $u$ receives inputs from all other neurons, and in turn, all other neurons
 receive the output of neuron $u$. 
\begin{itemize}
\item The network input function is given by
    \[
  \forall u \in U : \quad f^{(u)}_{\text{net}}(w_u, in_u) = \sum_{v \in U - \{u\}} w_{uv} \cdot \text{out}_v.
  \]
%The activation function for each neuron \( u \) is a threshold function:
\item The activation function is a threshold function
\[\forall u \in U : \quad f^{(u)}_{\text{act}}(\text{net}_u, \theta_u) =
  \begin{cases} 
    1 & \text{if } \text{net}_u \geq \theta_u, \\
    -1 & \text{otherwise}.
  \end{cases}
  \]
% The output function of each neuron is simply the identity function:
\item  The output function is the identity
\[\forall u \in U : \quad f^{(u)}_{\text{out}}(\text{act}_u) = \text{act}_u.
  \]
\end{itemize}
\end{definition}

\begin{theorem}[Convergence Theorem for Hopfield networks (Theorem 8.1, \cite{comp})] \label{Convergence}
\lean{hopfieldNet_convergence_fair}
\uses{HopfieldNetwork}
\leanok  
If the activations of the neurons of a Hopfield network are updated asynchronously,
a stable state is reached in a finite number of steps.
\end{theorem}

\begin{theorem}[Corollary of convergence Theorem for Hopfield networks (Theorem 8.1, \cite{comp})] \label{ConvergenceCor}
\lean{hopfieldNet_convergence_cyclic}
\uses{HopfieldNetwork}

\leanok  
If the neurons are 
traversed in an arbitrary, but fixed cyclic fashion, at most $n\cdot2^n$ steps
 (updates of individual neurons) are needed, where $n$ is the number of neurons of the network.
\end{theorem}

\begin{definition}
\lean{AsymmetricHopfieldNetwork}
\uses{HopfieldNetwork}
\leanok  
We define asymmetric Hopfield networks as general neural networks with the same graph514
structure and functions as symmetric Hopfield networks but with this matrix decomposition515
property instead of symmetry.
\end{definition}

\begin{definition}
\lean{potentialFunction}
\uses{HopfieldNetwork}
The potential function for asymmetric Hopfield networks at time step $k$
represents the energy of the network at time step $k$, considering that neuron $(k \mod n)$ is being updated.
\leanok
\end{definition}

\begin{lemma}
\lean{potential_function_bounded}
\uses{potentialFunction,HopfieldNetwork}
\leanok
The significance of this potential function lies in its relationship to Lyapunov stability566
theory. We prove it is bounded regardless of the network’s configuration.
\end{definition}

\begin{definition}
\lean{boltzmannDistribution}
\leanok
The Boltzmann distribution:
$$P(s) = \frac{e^{-E(s)/T}}{Z}$$
where $E(s)$ is the energy of state $s,$ $T$ is the temperature parameter and 
$Z = \sum_{s} e^{-E(s)/T}$ is the partition function.
\end{definition}


\begin{definition}
\lean{NN.State.gibbsUpdateSingleNeuron}
\uses{HopfieldNetwork,boltzmannDistribution}
\leanok
For neuron updates, we use Gibbs sampling, as introduced by Geman and Geman 
\cite{geman}, where a neuron $u$ is updated according to :

$$P(s_u = +1 | s_{-u}) = \frac{1}{1 + e^{-2h_u/T}}$$
where $h_u$ is the local field defined as $h_u = \sum_v w_{uv}s_v - \theta_u$. 

This formula can be derived directly from the Boltzmann distribution by 
considering the conditional probability of a single neuron's state given all others:

$$P(s_u = +1 | s_{-u}) = \frac{P(s_u = +1, s_{-u})}{P(s_u = +1, s_{-u}) + P(s_u = -1, s_{-u})}$$

The energy difference between states with $s_u = +1$ and $s_u = -1$ 
is $\Delta E = -2h_u$, which gives us the formula above after substitution and simplification.
\end{definition}

\begin{definition}
\lean{NN.State.simulatedAnnealing}
\leanok
We also implement simulated annealing, as introduced by Kirkpatrick et al. \cite{kirk},
which systematically decreases the temperature $T$ over time according to a cooling schedule:
$$T(t) = T_0 \times e^{-\alpha t}$$
where $T_0$ is the initial temperature and $\alpha$ is the cooling rate. 
\end{definition}

\begin{definition}
\lean{NN.State.metropolisHastingsStep}
\leanok
Another sampling method we formalize is the Metropolis-Hastings algorithm, 
introduced by Metropolis et al. \cite{metropolis} 
and later generalized by Hastings \cite{hastings}, which accepts or rejects 
proposed state changes with probability:
$$P(\text{accept}) = \min(1, e^{-(E(s') - E(s))/T})$$
where $s'$ is the proposed state after flipping a neuron. 
This allows the network to sometimes move to higher energy states, helping it escape local minima.
\end{definition}

\begin{definition}
\lean{totalVariationDistance}
\uses{boltzmannDistribution}
\leanok
To measure convergence to the equilibrium Boltzmann distribution,
 we use the total variation distance, as described by Levin and Peres \cite{levin} : 
$$d_{TV}(\mu, \nu) = \frac{1}{2} \sum_s |\mu(s) - \nu(s)|.$$
\end{definition}