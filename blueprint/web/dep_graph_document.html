

<!DOCTYPE html>
<html>
<head>
  <title>Dependency graph</title>
  <meta name="generator" content="plasTeX" />
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="styles/theme-white.css" />
  <link rel="stylesheet" href="styles/dep_graph.css" />
  
  <script type="text/x-mathjax-config">
  
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  
  </script>

  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">  </script>


<link rel="stylesheet" href="styles/extra_styles.css" />

</head>

<body>
<header>
  <a class="toc" href="index.html">Home</a>
  <h1 id="doc_title">Dependencies</h1>
</header>
<div class="wrapper">
<div class="content">
  <div id="Legend">
    <span id="legend_title" class="title">Legend
    <div class="btn">
       <div class="bar"></div>
       <div class="bar"></div>
       <div class="bar"></div>
    </div></span> 
    <dl class="legend">
      
      <dt>Boxes</dt><dd>definitions</dd>
      
      <dt>Ellipses</dt><dd>theorems and lemmas</dd>
      
      <dt>Blue border</dt><dd>the <em>statement</em> of this result is ready to be formalized; all prerequisites are done</dd>
      
      <dt>Orange border</dt><dd>the <em>statement</em> of this result is not ready to be formalized; the blueprint needs more work</dd>
      
      <dt>Blue background</dt><dd>the <em>proof</em> of this result is ready to be formalized; all prerequisites are done</dd>
      
      <dt>Green border</dt><dd>the <em>statement</em> of this result is formalized</dd>
      
      <dt>Green background</dt><dd>the <em>proof</em> of this result is formalized</dd>
      
      <dt>Dark green background</dt><dd>the <em>proof</em> of this result and all its ancestors are formalized</dd>
      
      <dt>Dark green border</dt><dd>this is in Mathlib</dd>
      
    </dl>
  </div>
    <div id="graph"></div>
<div id="statements">

    
    <div class="dep-modal-container" id="AbstractCompletionProbability_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="AbstractCompletionProbability" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">32</span></div>
    <div class="thm_thmcontent"><p>  The ‘AbstractCompletionProbability‘ measures the likelihood of correctly completing a pattern as a function of the Hamming distance ‘d‘ from the stored pattern. This formalizes the empirical finding from the paper (p.2556): "For distance \(\leq 5,\) the nearest state was reached more than 90% of the time. Beyond that distance, the probability fell off smoothly." </p>
</div>

    <a class="latex_link" href="sect0002.html#AbstractCompletionProbability">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="AbstractFamiliarityMeasure_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="AbstractFamiliarityMeasure" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">38</span></div>
    <div class="thm_thmcontent"><p>  ‘AbstractFamiliarityMeasure‘ quantifies how familiar a given state ‘s‘ is to the network, based on its proximity (closest Hamming distance) to stored patterns, using a provided ‘decay_func‘. </p>
<p>The paper discusses (p.2557): "The state 00000... is always stable. For a threshold of 0, this stable state is much higher in energy than the stored memory states and very seldom occurs." A high familiarity measure (close to 1) indicates ‘s‘ is similar to a stored pattern. The ‘ExponentialDecayMetric‘ can be used as ‘decay_func‘. </p>
</div>

    <a class="latex_link" href="sect0002.html#AbstractFamiliarityMeasure">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="Asymmetric HopfieldNetwork_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="Asymmetric HopfieldNetwork" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">5</span></div>
    <div class="thm_thmcontent"><p>   We define asymmetric Hopfield networks as general neural networks with the same graph514 structure and functions as symmetric Hopfield networks but with this matrix decomposition515 property instead of symmetry. </p>
</div>

    <a class="latex_link" href="sect0001.html#Asymmetric HopfieldNetwork">LaTeX</a>
    
    
  <a class="lean_link lean_decl" href="https://mkaratarakis.github.io/HopfieldNet/docs/find/#doc/AsymmetricHopfieldNetwork">Lean</a>
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="basin_volume_bound_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="basin_volume_bound" style="display: none">
    <div class="thm_thmheading">
      <span class="theorem_thmcaption">
      Theorem
      </span>
      <span class="theorem_thmlabel">37</span></div>
    <div class="thm_thmcontent"><p>  The ‘basin_volume_bound‘ theorem establishes that the basin volume decreases exponentially with the number of stored patterns. This formalizes how memory capacity affects retrieval robustness. </p>
</div>

    <a class="latex_link" href="sect0002.html#basin_volume_bound">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="BasinOfAttraction_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="BasinOfAttraction" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">19</span></div>
    <div class="thm_thmcontent"><p>  A ‘BasinOfAttraction‘ of a fixed point is the set of all states that converge to it. In the paper (p.2554): "Then, if the system is started sufficiently near any Xa, as at \(X = Xa + \Delta ,\) it will proceed in time until \(X \equiv Xa.\)" </p>
</div>

    <a class="latex_link" href="sect0002.html#BasinOfAttraction">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="BasinOfAttraction'_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="BasinOfAttraction'" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">35</span></div>
    <div class="thm_thmcontent"><p>  The ‘BasinOfAttraction’‘ of a pattern is the set of all states that converge to it. This concept is central to understanding the storage and retrieval properties of Hopfield networks. </p>
<p>From the paper (p.2554): "Then, if the system is started sufficiently near any Xa, as at \(X = Xa + \Delta ,\) it will proceed in time until \(X \equiv Xa.\) </p>
</div>

    <a class="latex_link" href="sect0002.html#BasinOfAttraction'">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="BasinVolume_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="BasinVolume" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">36</span></div>
    <div class="thm_thmcontent"><p>  The ‘BasinVolume‘ is the "size" of the basin of attraction, measured as the fraction of the state space that converges to a given pattern. </p>
<p>This quantifies the robustness of memory retrieval. </p>
</div>

    <a class="latex_link" href="sect0002.html#BasinVolume">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="boltzmannDistribution_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="boltzmannDistribution" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">8</span></div>
    <div class="thm_thmcontent"><p>  The Boltzmann distribution: </p>
<div class="displaymath" id="a0000000010">
  \[ P(s) = \frac{e^{-E(s)/T}}{Z} \]
</div>
<p> where \(E(s)\) is the energy of state \(s,\) \(T\) is the temperature parameter and \(Z = \sum _{s} e^{-E(s)/T}\) is the partition function. </p>
</div>

    <a class="latex_link" href="sect0001.html#boltzmannDistribution">LaTeX</a>
    
    
  <a class="lean_link lean_decl" href="https://mkaratarakis.github.io/HopfieldNet/docs/find/#doc/boltzmannDistribution">Lean</a>
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="bound_cross_talk_term_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="bound_cross_talk_term" style="display: none">
    <div class="thm_thmheading">
      <span class="lemma_thmcaption">
      Lemma
      </span>
      <span class="lemma_thmlabel">59</span></div>
    <div class="thm_thmcontent"><p>  Placeholder lemma for bounding the cross-talk term. Proving a tight bound likely requires assumptions beyond simple orthogonality, such as patterns being random and uncorrelated, analyzed in the limit of large N. The bound likely depends on ‘m‘ (number of patterns) and ‘N‘ (number of neurons). **Hopfield Assumption:** Implicitly assumes patterns behave statistically like random vectors. </p>
</div>

    <a class="latex_link" href="sect0002.html#bound_cross_talk_term">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="bound_cross_talk_term_abs_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="bound_cross_talk_term_abs" style="display: none">
    <div class="thm_thmheading">
      <span class="lemma_thmcaption">
      Lemma
      </span>
      <span class="lemma_thmlabel">62</span></div>
    <div class="thm_thmcontent"><p>  For random orthogonal patterns, the cross-talk term has a bounded absolute value. This is a fundamental assumption from Hopfield’s paper about how patterns interact. </p>
</div>

    <a class="latex_link" href="sect0002.html#bound_cross_talk_term_abs">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="commute_delete_foldl_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="commute_delete_foldl" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">51</span></div>
    <div class="thm_thmcontent"><p>  Helper lemma: DeleteNeuron commutes with foldl of DeleteNeuron if the neuron is not in the list. </p>
</div>

    <a class="latex_link" href="sect0002.html#commute_delete_foldl">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="ContentAddressableMemory_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="ContentAddressableMemory" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">29</span></div>
    <div class="thm_thmcontent"><p>  A Content Addressable Memory is a system that can retrieve a complete pattern from a partial or corrupted version. </p>
<p>This formalizes the central concept from the paper (p.2554): "A general content-addressable memory would be capable of retrieving this entire memory item on the basis of sufficient partial information. </p>
</div>

    <a class="latex_link" href="sect0002.html#ContentAddressableMemory">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="Convergence_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="Convergence" style="display: none">
    <div class="thm_thmheading">
      <span class="theorem_thmcaption">
      Theorem
      </span>
      <span class="theorem_thmlabel">3</span><span class="theorem_thmtitle">Convergence Theorem for Hopfield networks (Theorem 8.1, <span class="cite">
	[
	<a href="sect0003.html#comp" >5</a>
	]
</span>)</span></div>
    <div class="thm_thmcontent"><p>    If the activations of the neurons of a Hopfield network are updated asynchronously, a stable state is reached in a finite number of steps. </p>
</div>

    <a class="latex_link" href="sect0001.html#Convergence">LaTeX</a>
    
    
  <a class="lean_link lean_decl" href="https://mkaratarakis.github.io/HopfieldNet/docs/find/#doc/HopfieldNet_convergence_fair">Lean</a>
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="convergence_to_fixed_point_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="convergence_to_fixed_point" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">22</span></div>
    <div class="thm_thmcontent"><p>  This theorem captures the convergence result from the paper: "Every initial state flows to a limit point (if synchrony is not assumed)." </p>
</div>

    <a class="latex_link" href="sect0002.html#convergence_to_fixed_point">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="ConvergenceCor_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="ConvergenceCor" style="display: none">
    <div class="thm_thmheading">
      <span class="theorem_thmcaption">
      Theorem
      </span>
      <span class="theorem_thmlabel">4</span><span class="theorem_thmtitle">Corollary of convergence Theorem for Hopfield networks (Theorem 8.1, <span class="cite">
	[
	<a href="sect0003.html#comp" >5</a>
	]
</span>)</span></div>
    <div class="thm_thmcontent"><p>    If the neurons are traversed in an arbitrary, but fixed cyclic fashion, at most \(n\cdot 2^n\) steps (updates of individual neurons) are needed, where \(n\) is the number of neurons of the network. </p>
</div>

    <a class="latex_link" href="sect0001.html#ConvergenceCor">LaTeX</a>
    
    
  <a class="lean_link lean_decl" href="https://mkaratarakis.github.io/HopfieldNet/docs/find/#doc/HopfieldNet_convergence_cyclic">Lean</a>
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="cross_talk_term_abs_bound_assumption_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="cross_talk_term_abs_bound_assumption" style="display: none">
    <div class="thm_thmheading">
      <span class="lemma_thmcaption">
      Lemma
      </span>
      <span class="lemma_thmlabel">57</span></div>
    <div class="thm_thmcontent"><p>  Axiom stating the bound on the absolute value of the cross-talk term. This encapsulates the statistical argument from Hopfield’s paper that for random-like patterns, the sum of interfering terms is bounded. </p>
</div>

    <a class="latex_link" href="sect0002.html#cross_talk_term_abs_bound_assumption">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="delete_cons_neuron_step_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="delete_cons_neuron_step" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">48</span></div>
    <div class="thm_thmcontent"><p>  When deleting a list of neurons with a new neuron added at the front, the effect on the weighted sum equals the effect of deleting the first neuron and then deleting the rest of the list. </p>
</div>

    <a class="latex_link" href="sect0002.html#delete_cons_neuron_step">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="delete_empty_neurons_step_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="delete_empty_neurons_step" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">47</span></div>
    <div class="thm_thmcontent"><p> When deleting neurons from an empty list, the result is the original network  </p>
</div>

    <a class="latex_link" href="sect0002.html#delete_empty_neurons_step">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="delete_neuron_from_deleted_network_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="delete_neuron_from_deleted_network" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">50</span></div>
    <div class="thm_thmcontent"><p>  The effect of deleting a neuron from an already deleted network on a neuron u that is not in the deleted set. </p>
</div>

    <a class="latex_link" href="sect0002.html#delete_neuron_from_deleted_network">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="delete_neurons_recursive_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="delete_neurons_recursive" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">53</span></div>
    <div class="thm_thmcontent"><p>  Helper lemma: Deleting a list of neurons recursively subtracts their contributions. Requires that the list of neurons has no duplicates. </p>
</div>

    <a class="latex_link" href="sect0002.html#delete_neurons_recursive">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="delete_single_neuron_step_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="delete_single_neuron_step" style="display: none">
    <div class="thm_thmheading">
      <span class="lemma_thmcaption">
      Lemma
      </span>
      <span class="lemma_thmlabel">46</span></div>
    <div class="thm_thmcontent"><p>  When deleting a single neuron from a network, the resulting weighted sum for a neuron u that’s not the deleted neuron equals the original weighted sum minus the contribution from the deleted neuron. </p>
</div>

    <a class="latex_link" href="sect0002.html#delete_single_neuron_step">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="delete_singleton_neuron_step_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="delete_singleton_neuron_step" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">49</span></div>
    <div class="thm_thmcontent"><p>  For a singleton list, the effect matches the single neuron deletion case </p>
</div>

    <a class="latex_link" href="sect0002.html#delete_singleton_neuron_step">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="deleted_field_bound_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="deleted_field_bound" style="display: none">
    <div class="thm_thmheading">
      <span class="lemma_thmcaption">
      Lemma
      </span>
      <span class="lemma_thmlabel">60</span></div>
    <div class="thm_thmcontent"><p>  The field reduction from deleting neurons has a bounded effect. This version uses the decomposition and the (unproven) cross-talk bound. </p>
</div>

    <a class="latex_link" href="sect0002.html#deleted_field_bound">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="deleted_field_product_bound_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="deleted_field_product_bound" style="display: none">
    <div class="thm_thmheading">
      <span class="lemma_thmcaption">
      Lemma
      </span>
      <span class="lemma_thmlabel">63</span></div>
    <div class="thm_thmcontent"><p>  TO DO </p>
</div>

    <a class="latex_link" href="sect0002.html#deleted_field_product_bound">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="deleted_neurons_field_effect_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="deleted_neurons_field_effect" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">54</span></div>
    <div class="thm_thmcontent"><p>  DeleteNeurons removes weights connected to deleted neurons </p>
</div>

    <a class="latex_link" href="sect0002.html#deleted_neurons_field_effect">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="DeleteNeuron_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="DeleteNeuron" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">43</span></div>
    <div class="thm_thmcontent"><p>  The ‘DeleteNeuron‘ function simulates the failure of a neuron by removing its connections. This corresponds to setting weights to/from that neuron to zero. The paper discusses (p.2558): "The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices." </p>
</div>

    <a class="latex_link" href="sect0002.html#DeleteNeuron">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="DeleteNeurons_with_Finset_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="DeleteNeurons_with_Finset" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">69</span></div>
    <div class="thm_thmcontent"><p>  When deleting neurons from a Finset, we can use Finset.toList to convert the Finset to a List. This matches the API needed by DeleteNeurons. </p>
</div>

    <a class="latex_link" href="sect0002.html#DeleteNeurons_with_Finset">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="energy_convergence_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="energy_convergence" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">27</span></div>
    <div class="thm_thmcontent"><p>  The ‘energy_convergence‘ theorem formalizes the connection between energy minimization and the convergence to fixed points. This theorem establishes the connection between energy minimization and convergence to fixed points in Hopfield networks, as described in the paper From the paper (p.2555): "State changes will continue until a least (local) E is reached." </p>
</div>

    <a class="latex_link" href="sect0002.html#energy_convergence">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="EnergyChange_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="EnergyChange" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">21</span></div>
    <div class="thm_thmcontent"><p>  The ‘EnergyChange‘ when updating neuron i is always non-positive, as proven in the paper with Equation 8. </p>
<p>This theorem formalizes a key result from the paper: the energy function always decreases (or remains constant) under asynchronous updates. </p>
</div>

    <a class="latex_link" href="sect0002.html#EnergyChange">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="EnergyLandscape_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="EnergyLandscape" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">20</span></div>
    <div class="thm_thmcontent"><p>  The ‘EnergyLandscape‘ of a Hopfield network is the energy function defined over all possible states. In the paper, this is the function E defined in Equation 7: \( E = -1/2 \sum \sum Tij Vi Vj\) </p>
</div>

    <a class="latex_link" href="sect0002.html#EnergyLandscape">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="error_correction_guarantee_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="error_correction_guarantee" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">34</span></div>
    <div class="thm_thmcontent"><p>  The ‘error_correction_guarantee‘ theorem establishes that Hopfield networks can correct a substantial fraction of errors in the input pattern. </p>
<p>This formalizes a key capability of content-addressable memories. </p>
</div>

    <a class="latex_link" href="sect0002.html#error_correction_guarantee">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="ErrorCorrection_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="ErrorCorrection" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">33</span></div>
    <div class="thm_thmcontent"><p>  ‘ErrorCorrection‘ quantifies the network’s ability to correct errors in the input pattern. It’s measured as the reduction in Hamming distance to the closest stored pattern after convergence. </p>
</div>

    <a class="latex_link" href="sect0002.html#ErrorCorrection">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="ExponentialDecayMetric_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="ExponentialDecayMetric" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">31</span></div>
    <div class="thm_thmcontent"><p>  A specific exponential decay model, often used to model probabilities or familiarity scores. This corresponds to ‘exp(-value / (N/C))‘ where C is a constant (e.g., 10). </p>
</div>

    <a class="latex_link" href="sect0002.html#ExponentialDecayMetric">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="fault_tolerance_bound_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="fault_tolerance_bound" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">70</span></div>
    <div class="thm_thmcontent"><p>  A Hopfield network can tolerate the failure of up to 10% of its neurons while maintaining all stored patterns as fixed points, provided: </p>
<ol class="enumerate">
  <li><p>The stored patterns are orthogonal </p>
</li>
  <li><p>The number of patterns is at most 10% of the network size </p>
</li>
</ol>
</div>

    <a class="latex_link" href="sect0002.html#fault_tolerance_bound">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="FaultTolerance_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="FaultTolerance" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">44</span></div>
    <div class="thm_thmcontent"><p>  The ‘FaultTolerance‘ of a Hopfield network is its ability to maintain function despite the failure of some components. The paper notes that these networks are inherently robust to component failures. </p>
</div>

    <a class="latex_link" href="sect0002.html#FaultTolerance">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="field_remains_sufficient_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="field_remains_sufficient" style="display: none">
    <div class="thm_thmheading">
      <span class="lemma_thmcaption">
      Lemma
      </span>
      <span class="lemma_thmlabel">61</span></div>
    <div class="thm_thmcontent"><p> With constrained m and limited deleted neurons, the field remains strong enough </p>
</div>

    <a class="latex_link" href="sect0002.html#field_remains_sufficient">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="field_remains_sufficient_for_N_div_5_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="field_remains_sufficient_for_N_div_5" style="display: none">
    <div class="thm_thmheading">
      <span class="lemma_thmcaption">
      Lemma
      </span>
      <span class="lemma_thmlabel">64</span></div>
    <div class="thm_thmcontent"><p>  With bounded numbers of patterns and deleted neurons, the field remains strong enough to maintain the pattern stability, adjusted for N/5 bound. </p>
</div>

    <a class="latex_link" href="sect0002.html#field_remains_sufficient_for_N_div_5">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="FixedPoint_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="FixedPoint" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">18</span></div>
    <div class="thm_thmcontent"><p>  A ‘FixedPoint‘ of the phase space flow is a state that does not change under evolution. In the paper, these correspond to the locally stable states of the network (p.2554): "Various classes of flow patterns are possible, but the systems of use for memory particularly include those that flow toward locally stable points..." </p>
</div>

    <a class="latex_link" href="sect0002.html#FixedPoint">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="foldl_delete_preserves_weights_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="foldl_delete_preserves_weights" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">52</span></div>
    <div class="thm_thmcontent"><p>  Helper lemma: Weights are preserved by foldl if indices are not in the list </p>
</div>

    <a class="latex_link" href="sect0002.html#foldl_delete_preserves_weights">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="GibbsSampling_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="GibbsSampling" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">9</span></div>
    <div class="thm_thmcontent"><p>   For neuron updates, we use Gibbs sampling, as introduced by Geman and Geman <span class="cite">
	[
	<a href="sect0003.html#geman" >1</a>
	]
</span>, where a neuron \(u\) is updated according to : </p>
<div class="displaymath" id="a0000000011">
  \[ P(s_u = +1 | s_{-u}) = \frac{1}{1 + e^{-2h_u/T}} \]
</div>
<p> where \(h_u\) is the local field defined as \(h_u = \sum _v w_{uv}s_v - \theta _u\). </p>
<p>This formula can be derived directly from the Boltzmann distribution by considering the conditional probability of a single neuron’s state given all others: </p>
<div class="displaymath" id="a0000000012">
  \[ P(s_u = +1 | s_{-u}) = \frac{P(s_u = +1, s_{-u})}{P(s_u = +1, s_{-u}) + P(s_u = -1, s_{-u})} \]
</div>
<p>The energy difference between states with \(s_u = +1\) and \(s_u = -1\) is \(\Delta E = -2h_u\), which gives us the formula above after substitution and simplification. </p>
</div>

    <a class="latex_link" href="sect0001.html#GibbsSampling">LaTeX</a>
    
    
  <a class="lean_link lean_decl" href="https://mkaratarakis.github.io/HopfieldNet/docs/find/#doc/NN.State.gibbsUpdateSingleNeuron">Lean</a>
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="hebbian_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="hebbian" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">24</span></div>
    <div class="thm_thmcontent"><p>  The ‘hebbian‘ function computes the weight matrix according to Equation 2 of Hopfield’s paper: \(T_{ij} = \sum _{s} (2V_{i}^s - 1)(2V_{i}^s - 1) with T_{ii} = 0\) Note that this is equivalent to the existing ‘Hebbian‘ definition in HopfieldNet.HN, but we make the connection to the paper explicit here. </p>
</div>

    <a class="latex_link" href="sect0002.html#hebbian">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="hebbian_deleted_threshold_is_zero_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="hebbian_deleted_threshold_is_zero" style="display: none">
    <div class="thm_thmheading">
      <span class="lemma_thmcaption">
      Lemma
      </span>
      <span class="lemma_thmlabel">65</span></div>
    <div class="thm_thmcontent"><p>  TO DO </p>
</div>

    <a class="latex_link" href="sect0002.html#hebbian_deleted_threshold_is_zero">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="Hebbian_stable_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="Hebbian_stable" style="display: none">
    <div class="thm_thmheading">
      <span class="lemma_thmcaption">
      Lemma
      </span>
      <span class="lemma_thmlabel">58</span></div>
    <div class="thm_thmcontent"><p>  Decomposes the total sum representing the field reduction into the contribution from the target pattern ‘k‘ and the cross-talk term from other patterns \(l \neq k.\) </p>
<p>**Hopfield Assumption:** Assumes the standard Hebbian learning rule where \(T_{ii} = 0\). The ‘Hebbian‘ definition in ‘HN.lean‘ implements this by subtracting ‘m • 1‘. </p>
</div>

    <a class="latex_link" href="sect0002.html#Hebbian_stable">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="hebbian_weight_deleted_neurons_cross_talk_term_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="hebbian_weight_deleted_neurons_cross_talk_term" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">56</span></div>
    <div class="thm_thmcontent"><p>  Defines the cross-talk contribution to the deleted field sum. This term arises from the interaction of the target pattern ‘k‘ with other stored patterns \(`l \neq k`\) over the set of deleted neurons. </p>
</div>

    <a class="latex_link" href="sect0002.html#hebbian_weight_deleted_neurons_cross_talk_term">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="hebbian_weight_deleted_neurons_l_eq_k_term_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="hebbian_weight_deleted_neurons_l_eq_k_term" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">55</span></div>
    <div class="thm_thmcontent"><p>  Calculates the contribution to the deleted field sum from the target pattern ‘k‘ itself in the Hebbian weight definition. </p>
</div>

    <a class="latex_link" href="sect0002.html#hebbian_weight_deleted_neurons_l_eq_k_term">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="HopfieldNetwork_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="HopfieldNetwork" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">2</span></div>
    <div class="thm_thmcontent"><p>   A Hopfield network is a neural network with graph \(G = (U,C)\) as described in the previous section, that satisfies the following conditions: \( U_{\text{hidden}} = \emptyset \), and \( U_{\text{in}} = U_{\text{out}} = U \), \( C = U \times U - \{ (u, u) \mid u \in U \}  \), i.e., no self-connections. The connection weights are symmetric, i.e., for all \( u, v \in U \), we have \( w_{uv} = w_{vu} \) when \( u \neq v \). The activation of each neuron is either \( 1 \) or \( -1 \) depending on the input. There are no loops, meaning neurons don’t receive their own output as input. Instead, each neuron \(u\) receives inputs from all other neurons, and in turn, all other neurons receive the output of neuron \(u\). </p>
<ul class="itemize">
  <li><p>The network input function is given by </p>
<div class="displaymath" id="a0000000007">
  \[  \forall u \in U : \quad f^{(u)}_{\text{net}}(w_u, in_u) = \sum _{v \in U - \{ u\} } w_{uv} \cdot \text{out}_v.  \]
</div>
</li>
  <li><p>The activation function is a threshold function </p>
<div class="displaymath" id="a0000000008">
  \[ \forall u \in U : \quad f^{(u)}_{\text{act}}(\text{net}_u, \theta _u) = \begin{cases}  1 &  \text{if } \text{net}_u \geq \theta _u, \\ -1 &  \text{otherwise}. \end{cases}  \]
</div>
</li>
  <li><p>The output function is the identity </p>
<div class="displaymath" id="a0000000009">
  \[ \forall u \in U : \quad f^{(u)}_{\text{out}}(\text{act}_u) = \text{act}_u.  \]
</div>
</li>
</ul>
</div>

    <a class="latex_link" href="sect0001.html#HopfieldNetwork">LaTeX</a>
    
    
  <a class="lean_link lean_decl" href="https://mkaratarakis.github.io/HopfieldNet/docs/find/#doc/HopfieldNetwork">Lean</a>
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="isPseudoOrthogonal_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="isPseudoOrthogonal" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">25</span></div>
    <div class="thm_thmcontent"><p>  The ‘pseudoOrthogonality‘ property from Hopfield’s paper (Equations 3-4) states: For random patterns, the dot product between different patterns is approximately 0, while the dot product of a pattern with itself is approximately N. This property is essential for understanding the storage capacity of Hopfield networks. </p>
</div>

    <a class="latex_link" href="sect0002.html#isPseudoOrthogonal">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="localField_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="localField" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">15</span></div>
    <div class="thm_thmcontent"><p>  The ‘localField‘ for neuron i in state s is the weighted sum of inputs from other neurons, minus the threshold. This corresponds to \(\sum j Tij Vj - \theta _i\) in the paper. </p>
</div>

    <a class="latex_link" href="sect0002.html#localField">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="MetricDecayFunction_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="MetricDecayFunction" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">30</span></div>
    <div class="thm_thmcontent"><p> A generic function type representing how a metric (like completion probability or familiarity) decays or changes based on a distance-like value and network size. </p>
</div>

    <a class="latex_link" href="sect0002.html#MetricDecayFunction">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="Metropolis-Hastings_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="Metropolis-Hastings" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">11</span></div>
    <div class="thm_thmcontent"><p>   Another sampling method we formalize is the Metropolis-Hastings algorithm, introduced by Metropolis et al. <span class="cite">
	[
	<a href="sect0003.html#metropolis" >7</a>
	]
</span> and later generalized by Hastings <span class="cite">
	[
	<a href="sect0003.html#hastings" >2</a>
	]
</span>, which accepts or rejects proposed state changes with probability: </p>
<div class="displaymath" id="a0000000014">
  \[ P(\text{accept}) = \min (1, e^{-(E(s') - E(s))/T}) \]
</div>
<p> where \(s'\) is the proposed state after flipping a neuron. This allows the network to sometimes move to higher energy states, helping it escape local minima. </p>
</div>

    <a class="latex_link" href="sect0001.html#Metropolis-Hastings">LaTeX</a>
    
    
  <a class="lean_link lean_decl" href="https://mkaratarakis.github.io/HopfieldNet/docs/find/#doc/NN.State.metropolisHastingsStep">Lean</a>
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="net_input_at_non_deleted_neuron_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="net_input_at_non_deleted_neuron" style="display: none">
    <div class="thm_thmheading">
      <span class="lemma_thmcaption">
      Lemma
      </span>
      <span class="lemma_thmlabel">66</span></div>
    <div class="thm_thmcontent"><p>  TO DO </p>
</div>

    <a class="latex_link" href="sect0002.html#net_input_at_non_deleted_neuron">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="NeuralNetwork_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="NeuralNetwork" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">1</span></div>
    <div class="thm_thmcontent"><p>  An (artificial) neural network is a directed graph \( G = (U, C) \), where neurons \( u \in U \) are connected by directed edges \( c \in C \) (connections). The neuron set is partitioned as \( U = U_{\mathrm{in}} \cup U_{\mathrm{out}} \cup U_{\mathrm{hidden}} \), with \( U_{\mathrm{in}}, U_{\mathrm{out}} \neq \emptyset \) and \( U_{\mathrm{hidden}} \cap (U_{\mathrm{in}} \cup U_{\mathrm{out}}) = \emptyset \). Each connection \( (v, u) \in C \) has a weight \( w_{uv} \), and each neuron \( u \) has real-valued quantities: network input \( \mathrm{net}_u \), activation \( \mathrm{act}_u \), and output \( \mathrm{out}_u \). Input neurons \( u \in U_{\mathrm{in}} \) also have a fourth quantity, the external input \( \mathrm{ext}_u \). The predecessors and successors of a vertex \( u \) in a directed graph \( G = (U, C) \) are defined as \(\mathrm{pred}(u) = \{  v \in V \mid (v, u) \in C \} \) and \(\mathrm{succ}(u) = \{  v \in V \mid (u, v) \in C \} \) respectively. Each neuron \( u \) is associated with the following functions: </p>
<div class="displaymath" id="a0000000005">
  \[ f_{\mathrm{net}}^{(u)} : \mathbb {R}^{2|\mathrm{pred}(u)|+ \kappa _1 (u)} \to \mathbb {R}, \quad f_{\mathrm{act}}^{(u)} : \mathbb {R}^{1+\kappa _2 (u)} \to \mathbb {R}, \quad f_{\mathrm{out}}^{(u)} : \mathbb {R} \to \mathbb {R}.  \]
</div>
<p> These functions compute \( \mathrm{net}_u \), \( \mathrm{act}_u \), and \( \mathrm{out}_u \), where \( \kappa _1(u) \) and \( \kappa _2(u) \) count the number of parameters of those functions, which can depend on the neurons. Specifically, the new activation \(\mathrm{act}_u'\) of a neuron \(u\) is computed as follows: </p>
<div class="displaymath" id="a0000000006">
  \begin{equation*}  \mathrm{act}_u’= f_{\mathrm{act}}^{(u)} \big(f_{\mathrm{net}}^{(u)} \big( w_{uv_1}, \ldots , w_{uv_{\mathrm{pred}(u)}}, f_{\mathrm{out}}^{(v_1)}(\mathrm{act}_{v_1}),\ldots , f_{\mathrm{out}}^{(v_{\mathrm{pred}(u)})}(\mathrm{act}_{v_{\mathrm{pred}(u)}}), \boldsymbol {\sigma }^{(u)}\big), \boldsymbol {\theta }^{(u)}\big) \end{equation*}
</div>
<p> where \(\boldsymbol {\sigma }^{(u)} = (\sigma _1^{(u)} , \ldots , \sigma _{\kappa _1(u)}^{(u)} )\) and \(\boldsymbol {\theta } = (\theta _1^{(u)} , \ldots , \theta _{\kappa _2(u)}^{(u)} )\) are the input parameter vectors. </p>
</div>

    <a class="latex_link" href="sect0001.html#NeuralNetwork">LaTeX</a>
    
    
  <a class="lean_link lean_decl" href="https://mkaratarakis.github.io/HopfieldNet/docs/find/#doc/NeuralNetwork">Lean</a>
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="non_deleted_neuron_maintains_sign_of_activation_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="non_deleted_neuron_maintains_sign_of_activation" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">68</span></div>
    <div class="thm_thmcontent"><p>  TO DO </p>
</div>

    <a class="latex_link" href="sect0002.html#non_deleted_neuron_maintains_sign_of_activation">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="normalizedPattern_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="normalizedPattern" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">23</span></div>
    <div class="thm_thmcontent"><p>  The ‘normalizedPattern‘ converts a neural state to a vector with -1/+1 values, matching the \((2V_i - 1)\) term from equation 2 in Hopfield’s paper. </p>
</div>

    <a class="latex_link" href="sect0002.html#normalizedPattern">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="pattern_stability_in_hebbian_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="pattern_stability_in_hebbian" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">45</span></div>
    <div class="thm_thmcontent"><p>  When m is at most a tenth of total neurons, each pattern is fixed point in the undamaged network </p>
</div>

    <a class="latex_link" href="sect0002.html#pattern_stability_in_hebbian">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="PatternRetrievalError_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="PatternRetrievalError" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">41</span></div>
    <div class="thm_thmcontent"><p>  The ‘PatternRetrievalError‘ function computes the probability of an error in pattern retrieval for a network storing m patterns. This increases as m approaches and exceeds 0.15N. This corresponds to the error probability P discussed in Equation 10 of the paper: \( P = \int _{\sigma }^{\infty } \frac{1}{\sqrt{2\pi }} e^{-x^2 / 2} \,  dx = \frac{1}{2} \left(1 - \operatorname {erf}\left(\frac{\sigma }{\sqrt{2}}\right)\right) \), where \( \sigma = \frac{N}{2\sqrt{nN}} \) and \( n \) is the number of patterns. </p>
</div>

    <a class="latex_link" href="sect0002.html#PatternRetrievalError">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="PhaseSpaceFlow_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="PhaseSpaceFlow" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">17</span></div>
    <div class="thm_thmcontent"><p>  A ‘PhaseSpaceFlow‘ describes how the system state evolves over time. It maps each point in phase space to its successor state after updating one neuron. From the paper (p.2554): "The equations of motion of the system describe a flow in state space." </p>
</div>

    <a class="latex_link" href="sect0002.html#PhaseSpaceFlow">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="PhaseSpacePoint_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="PhaseSpacePoint" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">14</span></div>
    <div class="thm_thmcontent"><p>  A ‘PhaseSpacePoint‘ represents a state in the phase space of the Hopfield system. In the paper, this corresponds to the instantaneous state of all neurons (p.2554): "A point in state space then represents the instantaneous condition of the system." </p>
</div>

    <a class="latex_link" href="sect0002.html#PhaseSpacePoint">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="Potential function is bounded_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="Potential function is bounded" style="display: none">
    <div class="thm_thmheading">
      <span class="lemma_thmcaption">
      Lemma
      </span>
      <span class="lemma_thmlabel">7</span></div>
    <div class="thm_thmcontent"><p>   The significance of this potential function lies in its relationship to Lyapunov stability566 theory. We prove it is bounded regardless of the network’s configuration. </p>
</div>

    <a class="latex_link" href="sect0001.html#Potential function is bounded">LaTeX</a>
    
    
  <a class="lean_link lean_decl" href="https://mkaratarakis.github.io/HopfieldNet/docs/find/#doc/potential_function_bounded">Lean</a>
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="PotentialFunction_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="PotentialFunction" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">6</span></div>
    <div class="thm_thmcontent"><p>   The potential function for asymmetric Hopfield networks at time step \(k\) represents the energy of the network at time step \(k\), considering that neuron \((k \mod n)\) is being updated. </p>
</div>

    <a class="latex_link" href="sect0001.html#PotentialFunction">LaTeX</a>
    
    
  <a class="lean_link lean_decl" href="https://mkaratarakis.github.io/HopfieldNet/docs/find/#doc/potentialFunction">Lean</a>
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="product_net_input_activation_at_non_deleted_neuron_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="product_net_input_activation_at_non_deleted_neuron" style="display: none">
    <div class="thm_thmheading">
      <span class="lemma_thmcaption">
      Lemma
      </span>
      <span class="lemma_thmlabel">67</span></div>
    <div class="thm_thmcontent"><p>  TO DO </p>
</div>

    <a class="latex_link" href="sect0002.html#product_net_input_activation_at_non_deleted_neuron">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="Real.erf_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="Real.erf" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">40</span></div>
    <div class="thm_thmcontent"><p> The error function is defined as: </p>
<div class="displaymath" id="a0000000022">
  \[  \operatorname {erf}(x) = \frac{2}{\sqrt{\pi }} \int _0^x e^{-t^2} \,  dt  \]
</div>
<p> This function is central in probability theory, especially for normal distributions. </p>
</div>

    <a class="latex_link" href="sect0002.html#Real.erf">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="retrievalDistance_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="retrievalDistance" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">28</span></div>
    <div class="thm_thmcontent"><p>  The ‘retrievalDistance‘ function measures how far from a pattern we can initialize the network and still have it converge to that pattern. From the paper (p.2556): "For distance ≤ 5, the nearest state was reached more than 90% of the time. Beyond that distance, the probability fell off smoothly. </p>
</div>

    <a class="latex_link" href="sect0002.html#retrievalDistance">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="SimulatedAnnealing_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="SimulatedAnnealing" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">10</span></div>
    <div class="thm_thmcontent"><p>   We also implement simulated annealing, as introduced by Kirkpatrick et al. <span class="cite">
	[
	<a href="sect0003.html#kirk" >4</a>
	]
</span>, which systematically decreases the temperature \(T\) over time according to a cooling schedule: </p>
<div class="displaymath" id="a0000000013">
  \[ T(t) = T_0 \times e^{-\alpha t} \]
</div>
<p> where \(T_0\) is the initial temperature and \(\alpha \) is the cooling rate. </p>
</div>

    <a class="latex_link" href="sect0001.html#SimulatedAnnealing">LaTeX</a>
    
    
  <a class="lean_link lean_decl" href="https://mkaratarakis.github.io/HopfieldNet/docs/find/#doc/NN.State.simulatedAnnealing">Lean</a>
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="spin_glass_analogy_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="spin_glass_analogy" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">26</span></div>
    <div class="thm_thmcontent"><p>  The ‘spin_glass_analogy‘ formalizes the connection between Hopfield networks and physical spin glass systems, as discussed in the paper. </p>
<p>From the paper (p.2555): "This case is isomorphic with an Ising model." </p>
</div>

    <a class="latex_link" href="sect0002.html#spin_glass_analogy">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="stochasticHopfieldMarkovProcess_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="stochasticHopfieldMarkovProcess" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">13</span></div>
    <div class="thm_thmcontent"><p>   The stochastic Hopfield Markov process, which models the evolution of Hopfield network states over discrete time steps using Gibbs sampling at fixed temperature. In this simplified model, the transition kernel is time-homogeneous (same for all steps). </p>
</div>

    <a class="latex_link" href="sect0001.html#stochasticHopfieldMarkovProcess">LaTeX</a>
    
    
  <a class="lean_link lean_decl" href="https://mkaratarakis.github.io/HopfieldNet/docs/find/#doc/MarkovChain.stochasticHopfieldMarkovProcess">Lean</a>
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="storage_capacity_bound_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="storage_capacity_bound" style="display: none">
    <div class="thm_thmheading">
      <span class="theorem_thmcaption">
      Theorem
      </span>
      <span class="theorem_thmlabel">42</span></div>
    <div class="thm_thmcontent"><p>  The result from the paper that a Hopfield network can store approximately 0.15N patterns with high reliability, where \(N\) is the number of neurons. This theorem formalizes the key result about storage capacity from the paper, utilizing the Hebbian_stable theorem from the existing codebase. </p>
</div>

    <a class="latex_link" href="sect0002.html#storage_capacity_bound">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="StorageCapacity_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="StorageCapacity" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">39</span></div>
    <div class="thm_thmcontent"><p> The ‘StorageCapacity‘ of a Hopfield network is the maximum number of patterns that can be stored and reliably retrieved. The paper suggests this is around 0.15N. From the paper (p.2556): "About 0.15 N states can be simultaneously remembered before error in recall is severe." </p>
</div>

    <a class="latex_link" href="sect0002.html#StorageCapacity">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="Total variation_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="Total variation" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">12</span></div>
    <div class="thm_thmcontent"><p>   To measure convergence to the equilibrium Boltzmann distribution, we use the total variation distance, as described by Levin and Peres <span class="cite">
	[
	<a href="sect0003.html#levin" >6</a>
	]
</span> : </p>
<div class="displaymath" id="a0000000015">
  \[ d_{TV}(\mu , \nu ) = \frac{1}{2} \sum _s |\mu (s) - \nu (s)|. \]
</div>
</div>

    <a class="latex_link" href="sect0001.html#Total variation">LaTeX</a>
    
    
  <a class="lean_link lean_decl" href="https://mkaratarakis.github.io/HopfieldNet/docs/find/#doc/MarkovChain.totalVariation">Lean</a>
    
    
  
    
  </div>
    
      </div>
    </div>


    
    <div class="dep-modal-container" id="updateRule_modal">
      <div class="dep-modal-content">
          <button class="dep-closebtn">
<svg  class="icon icon-cross "><use xlink:href="symbol-defs.svg#icon-cross"></use></svg>
</button>
        
  <div class="thm" id="updateRule" style="display: none">
    <div class="thm_thmheading">
      <span class="definition_thmcaption">
      Definition
      </span>
      <span class="definition_thmlabel">16</span></div>
    <div class="thm_thmcontent"><p>  The ‘updateRule‘ defines the neural state update according to the paper’s Equation 1: \(Vi \rightarrow 1 if \sum j Tij Vj {\gt} Ui\) \( Vi \rightarrow 0 if \sum j Tij Vj {\lt} Ui\) In our formalization, we use -1 instead of 0 for the "not firing" state. </p>
</div>

    <a class="latex_link" href="sect0002.html#updateRule">LaTeX</a>
    
    
  
    
    
  
    
  </div>
    
      </div>
    </div>
</div>
</div> <!-- content -->
</div> <!-- wrapper -->
<script src="js/jquery.min.js" type="text/javascript"></script>

<script src="js/d3.min.js"></script>
<script src="js/hpcc.min.js"></script>
<script src="js/d3-graphviz.js"></script>

<script type="text/javascript">
const graphContainer = d3.select("#graph");
const width = graphContainer.node().clientWidth;
const height = graphContainer.node().clientHeight;


graphContainer.graphviz({useWorker: true})
    .width(width)
    .height(height)
    .fit(true)
    .renderDot(`strict digraph "" {	graph [bgcolor=transparent];	node [label="\N",		penwidth=1.8	];	edge [arrowhead=vee];	"Asymmetric HopfieldNetwork"	[color=green,		fillcolor="#B0ECA3",		label="Asymmetric HopfieldNetwork",		shape=box,		style=filled];	stochasticHopfieldMarkovProcess	[color=green,		fillcolor="#B0ECA3",		label=stochasticHopfieldMarkovProcess,		shape=box,		style=filled];	PhaseSpacePoint	[color=green,		fillcolor="#B0ECA3",		label=PhaseSpacePoint,		shape=box,		style=filled];	localField	[color=green,		fillcolor="#B0ECA3",		label=localField,		shape=box,		style=filled];	PhaseSpacePoint -> localField	[style=dashed];	FixedPoint	[color=green,		fillcolor="#B0ECA3",		label=FixedPoint,		shape=box,		style=filled];	PhaseSpacePoint -> FixedPoint	[style=dashed];	updateRule	[color=green,		fillcolor="#B0ECA3",		label=updateRule,		shape=box,		style=filled];	PhaseSpacePoint -> updateRule	[style=dashed];	EnergyChange	[color=green,		fillcolor="#B0ECA3",		label=EnergyChange,		shape=box,		style=filled];	PhaseSpacePoint -> EnergyChange	[style=dashed];	retrievalDistance	[color=green,		fillcolor="#B0ECA3",		label=retrievalDistance,		shape=box,		style=filled];	PhaseSpacePoint -> retrievalDistance	[style=dashed];	PhaseSpaceFlow	[color=green,		fillcolor="#B0ECA3",		label=PhaseSpaceFlow,		shape=box,		style=filled];	PhaseSpacePoint -> PhaseSpaceFlow	[style=dashed];	BasinOfAttraction	[color=green,		fillcolor="#B0ECA3",		label=BasinOfAttraction,		shape=box,		style=filled];	FixedPoint -> BasinOfAttraction	[style=dashed];	energy_convergence	[color=blue,		fillcolor="#A3D6FF",		label=energy_convergence,		shape=box,		style=filled];	FixedPoint -> energy_convergence	[style=dashed];	normalizedPattern	[color=green,		fillcolor="#B0ECA3",		label=normalizedPattern,		shape=box,		style=filled];	error_correction_guarantee	[color=green,		fillcolor="#B0ECA3",		label=error_correction_guarantee,		shape=box,		style=filled];	"Real.erf"	[color=green,		fillcolor="#B0ECA3",		label="Real.erf",		shape=box,		style=filled];	hebbian_weight_deleted_neurons_cross_talk_term	[color=green,		fillcolor="#B0ECA3",		label=hebbian_weight_deleted_neurons_cross_talk_term,		shape=box,		style=filled];	"Real.erf" -> hebbian_weight_deleted_neurons_cross_talk_term	[style=dashed];	field_remains_sufficient_for_N_div_5	[color=blue,		label=field_remains_sufficient_for_N_div_5,		shape=ellipse];	"Real.erf" -> field_remains_sufficient_for_N_div_5	[style=dashed];	PatternRetrievalError	[color=green,		fillcolor="#B0ECA3",		label=PatternRetrievalError,		shape=box,		style=filled];	"Real.erf" -> PatternRetrievalError	[style=dashed];	delete_neuron_from_deleted_network	[color=green,		fillcolor="#B0ECA3",		label=delete_neuron_from_deleted_network,		shape=box,		style=filled];	"Real.erf" -> delete_neuron_from_deleted_network	[style=dashed];	hebbian_weight_deleted_neurons_l_eq_k_term	[color=green,		fillcolor="#B0ECA3",		label=hebbian_weight_deleted_neurons_l_eq_k_term,		shape=box,		style=filled];	"Real.erf" -> hebbian_weight_deleted_neurons_l_eq_k_term	[style=dashed];	cross_talk_term_abs_bound_assumption	[color=blue,		label=cross_talk_term_abs_bound_assumption,		shape=ellipse];	hebbian_weight_deleted_neurons_cross_talk_term -> cross_talk_term_abs_bound_assumption	[style=dashed];	deleted_field_product_bound	[color=blue,		label=deleted_field_product_bound,		shape=ellipse];	hebbian_weight_deleted_neurons_cross_talk_term -> deleted_field_product_bound	[style=dashed];	bound_cross_talk_term	[color=blue,		label=bound_cross_talk_term,		shape=ellipse];	hebbian_weight_deleted_neurons_cross_talk_term -> bound_cross_talk_term	[style=dashed];	bound_cross_talk_term_abs	[color=green,		label=bound_cross_talk_term_abs,		shape=ellipse];	cross_talk_term_abs_bound_assumption -> bound_cross_talk_term_abs	[style=dashed];	fault_tolerance_bound	[color=green,		fillcolor="#B0ECA3",		label=fault_tolerance_bound,		shape=box,		style=filled];	deleted_field_product_bound -> fault_tolerance_bound	[style=dashed];	non_deleted_neuron_maintains_sign_of_activation	[label=non_deleted_neuron_maintains_sign_of_activation,		shape=box];	deleted_field_product_bound -> non_deleted_neuron_maintains_sign_of_activation	[style=dashed];	deleted_field_bound	[label=deleted_field_bound,		shape=ellipse];	bound_cross_talk_term -> deleted_field_bound	[style=dashed];	FaultTolerance	[color=green,		fillcolor="#B0ECA3",		label=FaultTolerance,		shape=box,		style=filled];	net_input_at_non_deleted_neuron	[color=blue,		label=net_input_at_non_deleted_neuron,		shape=ellipse];	product_net_input_activation_at_non_deleted_neuron	[label=product_net_input_activation_at_non_deleted_neuron,		shape=ellipse];	net_input_at_non_deleted_neuron -> product_net_input_activation_at_non_deleted_neuron	[style=dashed];	HopfieldNetwork	[color=green,		fillcolor="#B0ECA3",		label=HopfieldNetwork,		shape=box,		style=filled];	HopfieldNetwork -> "Asymmetric HopfieldNetwork"	[style=dashed];	HopfieldNetwork -> stochasticHopfieldMarkovProcess	[style=dashed];	HopfieldNetwork -> PhaseSpacePoint	[style=dashed];	HopfieldNetwork -> normalizedPattern	[style=dashed];	"Total variation"	[color=green,		fillcolor="#B0ECA3",		label="Total variation",		shape=box,		style=filled];	HopfieldNetwork -> "Total variation"	[style=dashed];	isPseudoOrthogonal	[color=green,		fillcolor="#B0ECA3",		label=isPseudoOrthogonal,		shape=box,		style=filled];	HopfieldNetwork -> isPseudoOrthogonal	[style=dashed];	ConvergenceCor	[color=green,		label=ConvergenceCor,		shape=ellipse];	HopfieldNetwork -> ConvergenceCor	[style=dashed];	AbstractFamiliarityMeasure	[color=green,		fillcolor="#B0ECA3",		label=AbstractFamiliarityMeasure,		shape=box,		style=filled];	HopfieldNetwork -> AbstractFamiliarityMeasure	[style=dashed];	PotentialFunction	[color=green,		fillcolor="#B0ECA3",		label=PotentialFunction,		shape=box,		style=filled];	HopfieldNetwork -> PotentialFunction	[style=dashed];	DeleteNeuron	[color=green,		fillcolor="#B0ECA3",		label=DeleteNeuron,		shape=box,		style=filled];	HopfieldNetwork -> DeleteNeuron	[style=dashed];	Convergence	[color=green,		label=Convergence,		shape=ellipse];	HopfieldNetwork -> Convergence	[style=dashed];	GibbsSampling	[color=green,		fillcolor="#B0ECA3",		label=GibbsSampling,		shape=box,		style=filled];	HopfieldNetwork -> GibbsSampling	[style=dashed];	SimulatedAnnealing	[color=green,		fillcolor="#B0ECA3",		label=SimulatedAnnealing,		shape=box,		style=filled];	HopfieldNetwork -> SimulatedAnnealing	[style=dashed];	EnergyLandscape	[color=green,		fillcolor="#B0ECA3",		label=EnergyLandscape,		shape=box,		style=filled];	HopfieldNetwork -> EnergyLandscape	[style=dashed];	storage_capacity_bound	[color=green,		label=storage_capacity_bound,		shape=ellipse];	HopfieldNetwork -> storage_capacity_bound	[style=dashed];	hebbian	[color=green,		fillcolor="#B0ECA3",		label=hebbian,		shape=box,		style=filled];	HopfieldNetwork -> hebbian	[style=dashed];	"Metropolis-Hastings"	[color=green,		fillcolor="#B0ECA3",		label="Metropolis-Hastings",		shape=box,		style=filled];	HopfieldNetwork -> "Metropolis-Hastings"	[style=dashed];	spin_glass_analogy	[color=blue,		fillcolor="#A3D6FF",		label=spin_glass_analogy,		shape=box,		style=filled];	HopfieldNetwork -> spin_glass_analogy	[style=dashed];	"Potential function is bounded"	[color=green,		label="Potential function is bounded",		shape=ellipse];	PotentialFunction -> "Potential function is bounded"	[style=dashed];	delete_empty_neurons_step	[color=green,		fillcolor="#B0ECA3",		label=delete_empty_neurons_step,		shape=box,		style=filled];	DeleteNeuron -> delete_empty_neurons_step	[style=dashed];	delete_single_neuron_step	[color=green,		label=delete_single_neuron_step,		shape=ellipse];	DeleteNeuron -> delete_single_neuron_step	[style=dashed];	hebbian_deleted_threshold_is_zero	[color=blue,		label=hebbian_deleted_threshold_is_zero,		shape=ellipse];	DeleteNeuron -> hebbian_deleted_threshold_is_zero	[style=dashed];	delete_cons_neuron_step	[color=green,		fillcolor="#B0ECA3",		label=delete_cons_neuron_step,		shape=box,		style=filled];	DeleteNeuron -> delete_cons_neuron_step	[style=dashed];	commute_delete_foldl	[color=green,		fillcolor="#B0ECA3",		label=commute_delete_foldl,		shape=box,		style=filled];	DeleteNeuron -> commute_delete_foldl	[style=dashed];	ExponentialDecayMetric	[color=green,		fillcolor="#B0ECA3",		label=ExponentialDecayMetric,		shape=box,		style=filled];	delete_neurons_recursive	[color=green,		fillcolor="#B0ECA3",		label=delete_neurons_recursive,		shape=box,		style=filled];	deleted_neurons_field_effect	[color=green,		fillcolor="#B0ECA3",		label=deleted_neurons_field_effect,		shape=box,		style=filled];	delete_neurons_recursive -> deleted_neurons_field_effect	[style=dashed];	deleted_neurons_field_effect -> net_input_at_non_deleted_neuron	[style=dashed];	convergence_to_fixed_point	[color=green,		fillcolor="#B0ECA3",		label=convergence_to_fixed_point,		shape=box,		style=filled];	AbstractCompletionProbability	[color=green,		fillcolor="#B0ECA3",		label=AbstractCompletionProbability,		shape=box,		style=filled];	foldl_delete_preserves_weights	[color=green,		fillcolor="#B0ECA3",		label=foldl_delete_preserves_weights,		shape=box,		style=filled];	foldl_delete_preserves_weights -> delete_neurons_recursive	[style=dashed];	DeleteNeurons_with_Finset	[color=blue,		fillcolor="#A3D6FF",		label=DeleteNeurons_with_Finset,		shape=box,		style=filled];	Hebbian_stable	[color=green,		label=Hebbian_stable,		shape=ellipse];	Hebbian_stable -> storage_capacity_bound	[style=dashed];	pattern_stability_in_hebbian	[color=green,		fillcolor="#B0ECA3",		label=pattern_stability_in_hebbian,		shape=box,		style=filled];	Hebbian_stable -> pattern_stability_in_hebbian	[style=dashed];	ErrorCorrection	[color=green,		fillcolor="#B0ECA3",		label=ErrorCorrection,		shape=box,		style=filled];	BasinVolume	[color=green,		fillcolor="#B0ECA3",		label=BasinVolume,		shape=box,		style=filled];	delete_singleton_neuron_step	[color=green,		fillcolor="#B0ECA3",		label=delete_singleton_neuron_step,		shape=box,		style=filled];	basin_volume_bound	[color=green,		label=basin_volume_bound,		shape=ellipse];	"BasinOfAttraction'"	[color=green,		fillcolor="#B0ECA3",		label="BasinOfAttraction'",		shape=box,		style=filled];	MetricDecayFunction	[color=green,		fillcolor="#B0ECA3",		label=MetricDecayFunction,		shape=box,		style=filled];	MetricDecayFunction -> ExponentialDecayMetric	[style=dashed];	MetricDecayFunction -> AbstractCompletionProbability	[style=dashed];	StorageCapacity	[color=green,		fillcolor="#B0ECA3",		label=StorageCapacity,		shape=box,		style=filled];	field_remains_sufficient	[color=green,		label=field_remains_sufficient,		shape=ellipse];	NeuralNetwork	[color=green,		fillcolor="#B0ECA3",		label=NeuralNetwork,		shape=box,		style=filled];	NeuralNetwork -> HopfieldNetwork	[style=dashed];	boltzmannDistribution	[color=green,		fillcolor="#B0ECA3",		label=boltzmannDistribution,		shape=box,		style=filled];	boltzmannDistribution -> "Total variation"	[style=dashed];	boltzmannDistribution -> GibbsSampling	[style=dashed];	ContentAddressableMemory	[color=green,		fillcolor="#B0ECA3",		label=ContentAddressableMemory,		shape=box,		style=filled];	ContentAddressableMemory -> error_correction_guarantee	[style=dashed];	ContentAddressableMemory -> ErrorCorrection	[style=dashed];	ContentAddressableMemory -> BasinVolume	[style=dashed];	ContentAddressableMemory -> basin_volume_bound	[style=dashed];	ContentAddressableMemory -> "BasinOfAttraction'"	[style=dashed];}`)
    .on("end", interactive);

latexLabelEscaper = function(label) {
  return label.replace(/\./g, '\\.').replace(/:/g, '\\:')
}

clickNode = function() {
  $("#statements div").hide()
  var node_id = $('text', this).text();
  $('.thm').hide();
  $('#'+latexLabelEscaper(node_id)).show().children().show();
}
function interactive() {
    $("span#legend_title").on("click", function () {
           $(this).siblings('dl').toggle();
        })

    d3.selectAll('.node')
        .attr('pointer-events', 'fill')
        .on('click', function () {
           var title = d3.select(this).selectAll('title').text().trim();
           $('#statements > div').hide()
           $('.thm').hide();
           $('#'+latexLabelEscaper(title)+'_modal').show().children().show().children().show();
           $('#statements').show()
        });

    d3.selectAll('.dep-closebtn').on('click', function() {
        var modal =
            d3.select(this).node().parentNode.parentNode.parentNode ;
        d3.select(modal).style('display', 'none');
    });
}

</script>

</body>
</html>
